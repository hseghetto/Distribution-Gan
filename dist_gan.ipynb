{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658673a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647994464312,
     "user": {
      "displayName": "Humberto Seghetto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00605077274056784693"
     },
     "user_tz": 180
    },
    "id": "658673a0",
    "outputId": "0d859c7a-0694-4bf9-a496-daba05cbfaed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb2bc4",
   "metadata": {
    "id": "fcdb2bc4"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a876c",
   "metadata": {
    "id": "386a876c"
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c257fd0c",
   "metadata": {
    "id": "c257fd0c"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3\n",
    "OUTPUT_SIZE = 3\n",
    "def generator_model():\n",
    "    input_shape=(INPUT_SIZE,)\n",
    "    input1 =  layers.Input(input_shape)\n",
    "\n",
    "    x = layers.Dense(64)(input1)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(OUTPUT_SIZE)(x)\n",
    "    \n",
    "    model = tf.keras.Model(input1,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e34af0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1647994465150,
     "user": {
      "displayName": "Humberto Seghetto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00605077274056784693"
     },
     "user_tz": 180
    },
    "id": "19e34af0",
    "outputId": "12900dbb-d83d-4195-fb7e-1e37b5d2608d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 8,771\n",
      "Trainable params: 8,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748e4c2",
   "metadata": {
    "id": "f748e4c2"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a0971d",
   "metadata": {
    "id": "65a0971d"
   },
   "outputs": [],
   "source": [
    "## input size do discriminador deve ser igual ao output size do gerador\n",
    "def discriminator_model():\n",
    "    input_shape = (OUTPUT_SIZE,)\n",
    "    data = layers.Input(input_shape)\n",
    "    \n",
    "    x = layers.Dense(64)(data)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Dense(1,activation = \"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(data,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bcbfb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1647994465707,
     "user": {
      "displayName": "Humberto Seghetto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00605077274056784693"
     },
     "user_tz": 180
    },
    "id": "05bcbfb3",
    "outputId": "99beb666-251b-4813-f309-fd54516a3bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,641\n",
      "Trainable params: 8,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908568a",
   "metadata": {
    "id": "8908568a"
   },
   "source": [
    "# Optimizers & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fac95a",
   "metadata": {
    "id": "63fac95a"
   },
   "outputs": [],
   "source": [
    "#Adjusted non-saturating gan losses\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(tf.math.log(fake_output+0.1))\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, penalty = 0):\n",
    "    return -tf.reduce_mean(tf.math.log(1-fake_output+0.1)) -tf.reduce_mean(tf.math.log(real_output+0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5051fbd4",
   "metadata": {
    "id": "5051fbd4"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f504c",
   "metadata": {
    "id": "cc8f504c"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254c6ead",
   "metadata": {
    "id": "254c6ead"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "#use this function to feed the generator\n",
    "def generate_input_noise(size):\n",
    "    #return tf.random.uniform([size,INPUT_SIZE],minval = 0,maxval = 1)\n",
    "    return tf.random.normal([size,INPUT_SIZE])\n",
    "\n",
    "#target distribution to be aproximated\n",
    "def generate_target_function(size, sigma = 1):\n",
    "    return np.exp(sigma*np.random.normal(size = size))\n",
    "\n",
    "def generate_and_save_histograms(epoch,test_input):\n",
    "    #use seed to generate predictions\n",
    "    predictions = generator(test_input, training = False)\n",
    "    #revert normalization\n",
    "    predictions = np.array(predictions)*STD+MEAN\n",
    "    \n",
    "    #Create figure\n",
    "    fig_size = (16,4)\n",
    "    fig, ax = plt.subplots(figsize=fig_size, nrows=1, ncols=OUTPUT_SIZE)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    #Plot generated and true distribution histograms together\n",
    "    for i in range(OUTPUT_SIZE):\n",
    "        ax[i].hist(predictions[:,i],bins = 32,histtype = \"step\", stacked = \"True\", linewidth=4)\n",
    "        ax[i].hist(generate_target_function(BATCH_SIZE,sigma[i]),\n",
    "                 color='red', histtype = \"step\", stacked = \"True\", bins = 32, linewidth=4)\n",
    "    \n",
    "    #Show and save figure\n",
    "    plt.savefig(image_log_dir+'\\epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e94f75",
   "metadata": {
    "id": "59e94f75"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-9-32493f2f9198>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-32493f2f9198>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    return gen_loss, disc_loss\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(data,labels):\n",
    "    noise = generate_input_noise(BATCH_SIZE)\n",
    "\n",
    "    print(\"Creating graph with:\",data.shape)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        print(\"Generating data\")\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        print(\"Calculating discriminator results\")\n",
    "        real_output = discriminator(data, training = True)\n",
    "        fake_output = discriminator(generated_data, training = True)\n",
    "        \n",
    "        print(\"Calculating loss\")\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, penalty=0)\n",
    "\n",
    "    print(\"Calculating gradient\")\n",
    "    gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    print(\"Applying gradient\")\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    print(\"Train step finished!\")\n",
    "    \n",
    "return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a648b2",
   "metadata": {
    "id": "44a648b2"
   },
   "outputs": [],
   "source": [
    "def train(Epochs):\n",
    "    print(\"Starting training\")\n",
    "    gen_loss=[]\n",
    "    disc_loss=[]\n",
    "    for epoch in range(1,Epochs+1):\n",
    "        start = time.time()\n",
    "\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        \n",
    "        g_loss = 0\n",
    "        d_loss = 0\n",
    "        for data_batch,labels in dataset:\n",
    "            g,d = train_step(data_batch,labels)\n",
    "            \n",
    "            g_loss = g + g_loss\n",
    "            d_loss = d + d_loss\n",
    "        \n",
    "        gen_loss.append(g_loss)\n",
    "        disc_loss.append(d_loss)\n",
    "        \n",
    "        if(epoch%20==0):\n",
    "            display.clear_output()\n",
    "            \n",
    "            plt.plot(gen_loss,label=\"Generator\")\n",
    "            plt.plot(disc_loss,label=\"Discriminator\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        if(epoch%20==0):\n",
    "            generate_and_save_histograms(epoch,seed)\n",
    "        \n",
    "        \n",
    "        print (\"Time for epoch {}/{} is {} sec\".format(epoch,Epochs, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3cff9",
   "metadata": {
    "id": "4fd3cff9"
   },
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc3aaa",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1647994466770,
     "user": {
      "displayName": "Humberto Seghetto",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00605077274056784693"
     },
     "user_tz": 180
    },
    "id": "b0bc3aaa",
    "outputId": "db80ef75-d0ea-437e-acb3-dbbab4221eb9"
   },
   "outputs": [],
   "source": [
    "set_size = 100*BATCH_SIZE\n",
    "dataset_size = OUTPUT_SIZE*set_size\n",
    "\n",
    "train_data = np.zeros([OUTPUT_SIZE,set_size]).astype(\"float32\")\n",
    "labels = np.ones([OUTPUT_SIZE,set_size]).astype(\"float32\")\n",
    "sigma = [1,0.5,0.1]\n",
    "for i in range(OUTPUT_SIZE):\n",
    "    labels[i] = labels[i]*sigma[i]\n",
    "    train_data[i] = generate_target_function(set_size,sigma[i])\n",
    "\n",
    "labels = labels.transpose()\n",
    "train_data = train_data.transpose()\n",
    "\n",
    "print(\"Dataset histogram\")\n",
    "plt.hist(train_data,bins = 100)\n",
    "plt.show() \n",
    "\n",
    "print(\"Dataset plot\")\n",
    "plt.plot(train_data)\n",
    "plt.show()\n",
    "\n",
    "#Normalization\n",
    "MEAN = np.mean(train_data)\n",
    "STD = np.std(train_data)  \n",
    "train_data = (train_data - MEAN)/STD\n",
    "\n",
    "#Discriminator/Generator test\n",
    "discriminator(generator(generate_input_noise(BATCH_SIZE)))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_data, labels))\n",
    "dataset = dataset.shuffle(2*BATCH_SIZE).batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4e3a0",
   "metadata": {
    "id": "f2e4e3a0"
   },
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05caf9",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3e05caf9",
    "outputId": "2836a552-5bf8-49a1-e4cc-5d1a369d9f7a"
   },
   "outputs": [],
   "source": [
    "seed = generate_input_noise(BATCH_SIZE)\n",
    "    \n",
    "generator = generator_model()\n",
    "discriminator = discriminator_model()\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "metric_log_dir = 'logs/metric/'+ current_time\n",
    "metric_summary_writer = tf.summary.create_file_writer(metric_log_dir)\n",
    "\n",
    "image_log_dir = 'logs/images/'+ current_time \n",
    "os.makedirs(image_log_dir)\n",
    "\n",
    "tf.keras.utils.plot_model(generator,to_file=image_log_dir+\"\\gen_gan.png\",show_shapes=True)\n",
    "tf.keras.utils.plot_model(discriminator,to_file=image_log_dir+\"\\disc_gan.png\",show_shapes=True)\n",
    "\n",
    "\n",
    "train(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd3d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "gan_dist.ipynb",
   "provenance": [
    {
     "file_id": "1kN4PopFfTJ2OmpkfckX1P4toMFZqBpYS",
     "timestamp": 1647989764806
    },
    {
     "file_id": "1S0Kb30sOt7CtgkbxVn2WX2Xmmc2lKUKy",
     "timestamp": 1647981299100
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
